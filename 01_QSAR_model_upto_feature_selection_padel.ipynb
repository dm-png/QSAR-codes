{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec74d77-35b4-4e66-ac9f-83b982a89867",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d83c80-146f-43dd-a633-51efa9f6f0fe",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a08965-8973-40f9-aecd-8fd7bc99e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7731271-ba31-4a4d-b967-e3144b6c8b34",
   "metadata": {},
   "source": [
    "### 2. Load and review the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86595a46-15d8-4bc2-a4e8-5ff69c35641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5156b-845c-403b-814b-cefd3bb9c364",
   "metadata": {},
   "source": [
    "### 3. Remove duplicates based on smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7bba1-394b-4654-8c49-29fa99b1ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['Smiles'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc969494-64df-4208-a445-6546e33d7000",
   "metadata": {},
   "source": [
    "### 4. Handling of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6f00d-9e14-424a-ac7f-cf5d093922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing pChEMBL Value\n",
    "df = df.dropna(subset=['pChEMBL Value'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4d6a2-c214-47ba-b6a0-a7f31cbb85f5",
   "metadata": {},
   "source": [
    "### 5. Outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a36e7-61c4-4a94-ae4a-0d56ac5a9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR for pChEMBL Value\n",
    "Q1 = df['pChEMBL Value'].quantile(0.25)\n",
    "Q3 = df['pChEMBL Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define upper and lower bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to remove outliers\n",
    "df = df[(df['pChEMBL Value'] >= lower_bound) & (df['pChEMBL Value'] <= upper_bound)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665f369-d15b-40c2-a9e5-77d2e568ffe7",
   "metadata": {},
   "source": [
    "### 6. Retain specific columns and rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb3f26-af00-4108-a5c9-6ac3c05c37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the specified columns\n",
    "df = df[['Compound Key', 'Smiles', 'pChEMBL Value']]\n",
    "\n",
    "# Rename the columns\n",
    "df.columns = ['Molecule', 'SMILES', 'pEC50']\n",
    "\n",
    "# Save the cleaned dataset with the desired columns and new names\n",
    "cleaned_file_path = '/pdel/cleaned_data.csv'  # specify your desired path\n",
    "df.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de21507-c0be-43a6-9bb7-c542c4e5806b",
   "metadata": {},
   "source": [
    "### 7. Distribution of pIC50 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bf81d-7d97-4316-9241-a220b3ecd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of seaborn\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a line plot for the probability distribution of pIC50 values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(df['pEC50'], fill=True, color='blue', alpha=0.2)  # Kernel Density Estimate\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Probability Distribution of pEC50 Values', fontsize=16)\n",
    "plt.xlabel('pEC50', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83199667-0200-47e4-83e8-c90a8744b110",
   "metadata": {},
   "source": [
    "### 8. Extracting the SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90076154-269f-4c5c-910b-551c1e5f6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =  df['SMILES']\n",
    "df2.to_csv('/padel/molecule.smi', index=False, header=False)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c041c-d4ba-44e6-b40b-dff48c22ea92",
   "metadata": {},
   "source": [
    "### 9. Calculating PaDEL descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d8865-6678-4d36-8933-89f935e7cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from padelpy import padeldescriptor\n",
    "descriptors = padeldescriptor(mol_dir='/padel/molecule.smi', d_file='CHEMBL941720/padel/descriptors.csv', d_2d=True, d_3d=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab8a79-43c0-428d-b6f4-1865830a07b0",
   "metadata": {},
   "source": [
    "### 10. Concatnetae descriptor data with molecules and pIC50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bacf32-e2e4-4e88-902b-391a8c46dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original DataFrame (df) with 'Molecule', 'SMILES', and 'pIC50' columns\n",
    "# Assuming df is already available in your environment\n",
    "\n",
    "# Load descriptors.csv, skip the first column\n",
    "descriptors_df = pd.read_csv('/padel/descriptors.csv').iloc[:, 1:]\n",
    "\n",
    "# Concatenate the original columns (Molecule, SMILES, pIC50) with descriptors\n",
    "# First, reset the index of `df` and `descriptors_df` to ensure they align correctly\n",
    "result_df = pd.concat([df[['Molecule', 'SMILES']].reset_index(drop=True), descriptors_df, df[['pEC50']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "result_df.to_csv('/padel/final_descriptors.csv', index=False)\n",
    "print(\"The new file 'final_descriptors.csv' has been created with the required columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94775d-a65d-4367-b5b9-b99609ea0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554121e-0138-4de4-bebd-4ccb62c12bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa72ebc-95d6-4042-b8fe-7e17fb938b13",
   "metadata": {},
   "source": [
    "### 11. Descriptor reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee37c8-8205-4a21-a365-762084cc8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(/padel/final_descriptors.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793a4a2-db85-42ad-a2a9-78cff27625f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with any missing values\n",
    "data = data.dropna(axis=1)\n",
    "\n",
    "# Step 1: Remove virtually constant columns (>95% same values)\n",
    "# Calculate the percentage of unique values in each column\n",
    "constant_columns = [col for col in data.columns[1:-1] if data[col].value_counts(normalize=True).values[0] > 0.95]\n",
    "data = data.drop(columns=constant_columns)\n",
    "\n",
    "# Step 2: Remove highly correlated columns (|r| > 0.90)\n",
    "# Compute the correlation matrix for the remaining descriptors\n",
    "corr_matrix = data.iloc[:, 2:-1].corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of columns with correlation greater than 0.90\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.90)]\n",
    "data = data.drop(columns=to_drop)\n",
    "\n",
    "# Display the results\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5769487-3960-4955-81b3-e6a19188e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3535fe4-c178-4f61-8258-474669a497bf",
   "metadata": {},
   "source": [
    "### 12. Feature Selection via RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0836a-a8c0-418d-8e1c-171047eb4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Separate the predictors and target variable\n",
    "X = data.iloc[:, 2:-1]  # Descriptors\n",
    "y = data.iloc[:, -1]     # pIC50\n",
    "\n",
    "# Define the model and RFE\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=8)  # Adjust to select your desired number of top features\n",
    "\n",
    "# Fit RFE to data\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d86163-7f6e-4227-803e-ea6b2c02daea",
   "metadata": {},
   "source": [
    "### 13. Feature selection via stepwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7715a-5fbb-4a6d-ba14-78b2b9327e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Load data\n",
    "\n",
    "X = data.iloc[:, 2:-1]  # Descriptors (assuming the first column is compound key)\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "def stepwise_selection(X, y, threshold_in=0.05, threshold_out=0.05, max_features=8):\n",
    "    included = []\n",
    "    while len(included) < max_features:\n",
    "        changed = False\n",
    "\n",
    "        # Forward step: Try adding features\n",
    "        excluded = list(set(X.columns) - set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "\n",
    "        for col in excluded:\n",
    "            model = sm.OLS(y, add_constant(X[included + [col]])).fit()\n",
    "            new_pval[col] = model.pvalues[col]\n",
    "\n",
    "        # Select the best feature with p-value below the threshold_in\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed = True\n",
    "\n",
    "        # Check if we've reached the max_features limit\n",
    "        if len(included) >= max_features:\n",
    "            break\n",
    "\n",
    "        # Backward step: Try removing features with p-value above the threshold_out\n",
    "        model = sm.OLS(y, add_constant(X[included])).fit()\n",
    "        pvalues = model.pvalues.iloc[1:]  # Skip intercept\n",
    "        worst_pval = pvalues.max()\n",
    "\n",
    "        if worst_pval > threshold_out:\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            changed = True\n",
    "\n",
    "        # Break if no changes occurred (i.e., stable subset reached)\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    # Final model to print the selected features' details\n",
    "    final_model = sm.OLS(y, add_constant(X[included])).fit()\n",
    "    print(final_model.summary())\n",
    "    return included\n",
    "\n",
    "# Run stepwise selection with a specific maximum number of features\n",
    "selected_features = stepwise_selection(X, y, max_features=8)\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2832e33-470f-44ce-9e79-fb405eaf556f",
   "metadata": {},
   "source": [
    "### 14. Building the multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4eb2d-3e59-4376-9672-0dd85f528866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Separate predictors and target variable based on selected descriptors\n",
    "X = data[['MDEC-33', 'VE1_Dzp', 'ATSC6e', 'minaaN', 'SpMax4_Bhm', 'nAtomLAC', 'VE3_Dzs']]\n",
    "y = data[\"pEC50\"]\n",
    "\n",
    "# Bin y into bins\n",
    "y_binned = pd.qcut(y, q=5, duplicates='drop') \n",
    "\n",
    "# Split data into training and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "\n",
    "# Initialize and fit the MLR model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05584a74-32ac-4c00-ae63-d00915c95a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean performance:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a513631-575c-44d5-b35b-930457f917a3",
   "metadata": {},
   "source": [
    "### 15. Save the dataset for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd889a0-1302-4e60-a233-2b870f10320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selected feature columns\n",
    "selected_features = ['MDEC-33', 'VE1_Dzp', 'ATSC6e', 'minaaN', 'SpMax4_Bhm', 'nAtomLAC', 'VE3_Dzs']\n",
    "\n",
    "# Ensure that 'Compound Key' and 'pIC50' are part of the data\n",
    "# Select the relevant columns from your dataset\n",
    "selected_columns = ['Molecule'] + selected_features + ['pEC50']\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "selected_data = data[selected_columns]\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "selected_data.to_csv(\"/padel/selected_features.csv\", index=False)\n",
    "\n",
    "print(\"Selected data saved to 'selected_features.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f273a00-d29c-4d1a-9b98-b25390c94053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
